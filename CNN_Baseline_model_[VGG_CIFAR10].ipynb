{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Baseline_model_[VGG-CIFAR10].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IbbfPYcBo7Lz"
      },
      "outputs": [],
      "source": [
        "# baseline model with dropout on the cifar10 dataset\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation test harness"
      ],
      "metadata": {
        "id": "EtdOUTn37hL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load dataset**\n",
        "\n",
        "We know that all the images have:\n",
        "- Same square size of 32x32 pixels\n",
        "- Labeled and divided into 10 classes\n",
        "- The images are color\n",
        "\n",
        "Therefore we can load the images and use them for modeling almost immediately."
      ],
      "metadata": {
        "id": "npAAiB4isKn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        " \n",
        "\t# one hot encode target values\n",
        "\ttrainY = np_utils.to_categorical(trainY)\n",
        "\ttestY = np_utils.to_categorical(testY)\n",
        " \n",
        "\treturn trainX, trainY, testX, testY"
      ],
      "metadata": {
        "id": "bZ5rII6QxWBW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pixel values for each image in the dataset are unsigned integers in the range between no color and full color (0 and 255). We need to scale the pixel values for modeling, but we don't know how exactly. To solve this problem, we can normalize the pixel values, rescale them to the range [0, 1], and because our data is intergers, so first we need to convert them into floats, then divide the pixel values by the maximum value."
      ],
      "metadata": {
        "id": "x-kUoQft0TrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        " \n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t\n",
        "\treturn train_norm, test_norm"
      ],
      "metadata": {
        "id": "-W0ZPvrUzPi6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step, we define our model. This step we will discuss later because it requires some knowledge and would take a little bit of time. \n",
        "Once the model has been evaluated, we can present the results:\n",
        "- The diagnostics of the learning behavior of the model during training, by creating a line spot showing model performance on the train and test set during training. These plots are valuable for getting an idea of whether a model is overfitting, underfitting, or has a good fit for the dataset.\n",
        "- The estimation of the model performance. We will create a single figure with two subplots, one for loss and one for accuracy. The blue lines will indicate model performance on the training dataset and orange lines will indicate performance on the hold out test dataset."
      ],
      "metadata": {
        "id": "byKqH5P84eez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        " \n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()"
      ],
      "metadata": {
        "id": "8aot7Qc05GDI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we implement the test harness to kick-off the evaluation of a given model."
      ],
      "metadata": {
        "id": "REo61xKN6EvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        " \n",
        "\t# prepare pixel data\n",
        "\ttrainX, testX = prep_pixels(trainX, testX)\n",
        " \n",
        "\t# define model\n",
        "\tprint('Defining machine learning model...')\n",
        "\tmodel = define_model()\n",
        " \n",
        "\t# fit model\n",
        "\tprint('Fitting machine learning model...')\n",
        "\thistory = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)\n",
        "\t\n",
        "\t# evaluate model\n",
        "\tprint('Evaluating machine learning model...')\n",
        "\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
        " \n",
        "  # Report final model performance on the test dataset\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\tsummarize_diagnostics(history)"
      ],
      "metadata": {
        "id": "on3atI7-55PY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Develop a baseline model\n",
        "\n",
        "Here we use VGG architecture with 3 blocks, for details:\n",
        "- The first block has 32 3x3 filters. This quantity should double after each block. We have 3 blocks, so the number of filters in each block should be 32, 63, 128 respectively with 'he_uniform' kernal_initializer.\n",
        "- Each block has 2 convolution layers with same padding and followed by ReLU activation function.\n",
        "- Each block has a 2x2 max pooling layer.\n",
        "\n",
        "After 3 convolution layers, we flatten the tensor output and pass it into a fully-connected layer with ReLU activation function. Finally, we use softmax function to get the prediction output.\n"
      ],
      "metadata": {
        "id": "Lbtc0I7E6ylE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define baseline cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        " \n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        " \n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        " \n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        " \n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        " \n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.001, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "Mro3CfKj7NMQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try our model!"
      ],
      "metadata": {
        "id": "YYwtLjIsgqSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_test_harness()"
      ],
      "metadata": {
        "id": "CMd3ivBdgqBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21808ad6-e93e-45dd-e7fc-149de0d35ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining machine learning model...\n",
            "Fitting machine learning model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    }
  ]
}